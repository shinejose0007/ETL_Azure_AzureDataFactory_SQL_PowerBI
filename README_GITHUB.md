# Demo: Azure Data Factory → Synapse → Power BI

This repository contains a small, end-to-end demo you can use to showcase:
- simple ETL with Python (etl.py)
- loading transactional data into Azure Synapse (create_table.sql)
- an Azure Data Factory pipeline template to copy CSV from Blob to Synapse
- building a Power BI dashboard (instructions + pbix_readme.txt)

How to use:
1. Run `python etl.py` to generate `transformed_daily_kpis.csv`.
2. (Optional) Upload `synapse_load.csv` to Azure Blob Storage and use the ADF pipeline to copy into Synapse.
3. Open Power BI Desktop and import `transformed_daily_kpis.csv` (see pbix_readme.txt for recommended visuals).
4. See `dax_measures.md` for suggested DAX measures to add to your Power BI model.

Repository contents:
- data_sample_sales.csv — sample transactional data
- etl.py — ETL script to produce KPI table
- transformed_daily_kpis.csv — (generated by running etl.py)
- create_table.sql — Synapse table creation script
- data_factory_pipeline_example.json — example pipeline template
- pbix_readme.txt — 3-sentence description for the Power BI artifact
- dax_measures.md — suggested DAX formulas and short explanations
- ADF linked service & dataset JSON placeholders (LinkedService_BlobStorage.json, LinkedService_Synapse.json, Dataset_Blob_CSV.json, Dataset_Synapse_Table.json, pipeline_filled.json)

Notes:
- Replace placeholder names in ADF JSON files with your actual Linked Service and dataset names before importing into ADF.
- The `.pbix` file is not included — create it locally with Power BI Desktop using `transformed_daily_kpis.csv` or connect it directly to Synapse after data load.
